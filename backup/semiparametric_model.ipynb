{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ec3606-654a-4892-a90a-2bd342b060fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from utils import *\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "def logit(x, eps=.001):\n",
    "    c = 0 #np.min(x)\n",
    "    x2 = (np.array(x)-c)/(1-c)\n",
    "    return np.log((x2+eps)/(1-x2+eps))\n",
    "\n",
    "data_path = '/llmthonskdir/felipe/download_openllmlb/'\n",
    "\n",
    "mmlu_subs = ['hendrycksTest-abstract_algebra', 'hendrycksTest-anatomy', 'hendrycksTest-astronomy', 'hendrycksTest-business_ethics',\n",
    "             'hendrycksTest-clinical_knowledge', 'hendrycksTest-college_biology', 'hendrycksTest-college_chemistry',\n",
    "             'hendrycksTest-college_computer_science', 'hendrycksTest-college_mathematics', 'hendrycksTest-college_medicine', 'hendrycksTest-college_physics', 'hendrycksTest-computer_security', 'hendrycksTest-conceptual_physics', 'hendrycksTest-econometrics', 'hendrycksTest-electrical_engineering', 'hendrycksTest-elementary_mathematics', 'hendrycksTest-formal_logic', 'hendrycksTest-global_facts', 'hendrycksTest-high_school_biology', 'hendrycksTest-high_school_chemistry', 'hendrycksTest-high_school_computer_science', 'hendrycksTest-high_school_european_history', 'hendrycksTest-high_school_geography', 'hendrycksTest-high_school_government_and_politics', 'hendrycksTest-high_school_macroeconomics', 'hendrycksTest-high_school_mathematics', 'hendrycksTest-high_school_microeconomics', 'hendrycksTest-high_school_physics', 'hendrycksTest-high_school_psychology', 'hendrycksTest-high_school_statistics', 'hendrycksTest-high_school_us_history', 'hendrycksTest-high_school_world_history', 'hendrycksTest-human_aging', 'hendrycksTest-human_sexuality', 'hendrycksTest-international_law', 'hendrycksTest-jurisprudence', 'hendrycksTest-logical_fallacies', 'hendrycksTest-machine_learning', 'hendrycksTest-management', 'hendrycksTest-marketing', 'hendrycksTest-medical_genetics', 'hendrycksTest-miscellaneous', 'hendrycksTest-moral_disputes', 'hendrycksTest-moral_scenarios', 'hendrycksTest-nutrition', 'hendrycksTest-philosophy', 'hendrycksTest-prehistory', 'hendrycksTest-professional_accounting', 'hendrycksTest-professional_law', 'hendrycksTest-professional_medicine', 'hendrycksTest-professional_psychology', 'hendrycksTest-public_relations', 'hendrycksTest-security_studies', 'hendrycksTest-sociology', 'hendrycksTest-us_foreign_policy', 'hendrycksTest-virology', 'hendrycksTest-world_religions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b420e81-fa76-4cf5-bd09-4aea2167a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/base_llm_benchmark_eval.csv')\n",
    "models_scaling = list(data.Model)\n",
    "models_scaling = [filter(m) for m in models_scaling]\n",
    "\n",
    "with open(data_path+'scaling_laws/old_leaderboard_processed_20240630.pickle', 'rb') as handle:\n",
    "    lb_data = pickle.load(handle)\n",
    "models_lb = lb_data['hendrycksTest-abstract_algebra']['models']\n",
    "models_lb = [filter(m) for m in models_lb]\n",
    "\n",
    "accs_mmlu = [lb_data[s]['correctness'].mean(1) for s in mmlu_subs]\n",
    "accs_mmlu = np.vstack(accs_mmlu).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ca3e81-7113-4f2f-ad10-fcc2ed1e3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path+'/old_leaderboard_processed_20240630.pickle', 'rb') as handle:\n",
    "    full_lb_data = pickle.load(handle)\n",
    "asymptot = {}\n",
    "for s in full_lb_data.keys():\n",
    "    asymptot[s] = np.percentile(full_lb_data[s]['correctness'].mean(-1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db40b128-8ede-487c-aaaf-49db421ed256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,s in enumerate(mmlu_subs):\n",
    "    posics = []\n",
    "    for m in models_scaling:\n",
    "        if m in models_lb:\n",
    "            posics.append(np.array(np.array(models_lb)==m).argmax())\n",
    "        else:\n",
    "            posics.append(-1)\n",
    "    \n",
    "    y = []\n",
    "    for posic in posics:\n",
    "        if posic==-1:\n",
    "            y.append(np.nan)\n",
    "        else:\n",
    "            y.append(accs_mmlu[posic,j])\n",
    "            \n",
    "    data[s] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d2d263-bfff-4ba4-b3bc-64a1fc6a10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fam_encoder = LabelEncoder()\n",
    "fam_encoder.fit(data['Model Family'])\n",
    "data['T'] = data['Pretraining Data Size (T)']\n",
    "data['S'] = data['Model Size (B)']\n",
    "data['F'] = data['FLOPs (1E21)']\n",
    "data['family'] = data['Model Family']\n",
    "data = data.sort_values(by=['family','S']).reset_index(drop=True)\n",
    "data['logT'] = np.log(data['T'])\n",
    "data['logS'] = np.log(data['S'])\n",
    "data['logF'] = np.log(data['F'])\n",
    "data['logS*logT'] = data['logS']*data['logT']\n",
    "data = data[['family','logT','logS','logF','logS*logT','ARC-C','HellaSwag','Winograd','TruthfulQA','GSM8K']+mmlu_subs] #,'XWinograd','HumanEval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cdcaa2-2853-4281-9d96-e4f46a9c9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how='any')\n",
    "unique_families, counts_families = np.unique(data.family, return_counts=True)\n",
    "avail_families = unique_families[counts_families>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6aa9057-72c0-428c-aa3e-6a24ae2943a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_family = avail_families[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7adc107a-c146-411f-82f3-751299f09b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.loc[data.family != test_family]\n",
    "data_test = data.loc[data.family == test_family]\n",
    "data_train = pd.concat((data_test.iloc[:1],data_train), axis=0).reset_index(drop=True)\n",
    "data_test = data_test.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "###\n",
    "Y_train = torch.tensor(np.array(data_train.loc[:,['ARC-C','HellaSwag','Winograd','TruthfulQA','GSM8K']+mmlu_subs]))\n",
    "X_train = np.array(data_train.loc[:,['logT','logS','logS*logT']])\n",
    "X_train = torch.tensor(X_train).double()\n",
    "Y_test = torch.tensor(np.array(data_test.loc[:,['ARC-C','HellaSwag','Winograd','TruthfulQA','GSM8K']+mmlu_subs]))\n",
    "X_test = np.array(data_test.loc[:,['logT','logS','logS*logT']])\n",
    "X_test = torch.tensor(X_test).double()\n",
    "D_train = torch.tensor(np.array(pd.get_dummies(np.array(data_train.family)))).double()\n",
    "D_test = torch.tensor(np.vstack([D_train[0,:].numpy() for _ in range(Y_test.shape[0])])).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8672b9-661e-4d49-af9d-24a6a51de382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "77cecb10-8124-4818-b48c-f44f9866112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([83, 3])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "78044bc5-6ea9-4fe9-aff8-f57664c0f828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e16801c-f25a-481f-a42d-9724026177d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k(x, h):\n",
    "    return torch.exp(-(x/h)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0ee7f146-be36-426f-911a-04486d11a758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([83, 62])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bd2ceaf0-baf9-4dc8-833c-cb2374cc9224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 62, 1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "580a4000-402b-46ab-89fc-67939da2fa22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8e0c6c23-c3ab-4204-b61e-9ede67a907e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "27fd9f7e-2092-4605-9b2d-04a6e0e06a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(K.shape, dtype=bool)\n",
    "for i in range(mask.shape[0]):\n",
    "    mask[i,:,i] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d59013ae-49e6-40a4-addc-4b2c8164d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = torch.nn.Parameter(torch.normal(0, .1, size=(D_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "beta = torch.nn.Parameter(torch.normal(0, .1, size=(X_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "h=.1*torch.nn.Parameter(torch.ones((1,Y_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "\n",
    "#training loop here\n",
    "\n",
    "Z = (X_train@beta)+(D_train@thetas)\n",
    "Z = Z@torch.ones((1,Y_train.shape[1])).double()\n",
    "K = k(Z[:,:,None]-Z.T[None,:,:], h)\n",
    "\n",
    "loss = 0\n",
    "for i in range(mask.shape[0]):\n",
    "    Y_hat = ((((K*Y_train.T[None,:,:])[i])*mask[i]).sum(1)/((K*Y_train.T[None,:,:])[i]).sum(1))\n",
    "    loss += (Y_train[i]-Y_hat)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd442d9-edb1-4d7d-8b25-de04a83ef839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "eecb70a2-6cb9-4a56-9975-dcf8c69e344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1961.9933985484663\n",
      "Epoch 2/100, Loss: 1918.174402303216\n",
      "Epoch 3/100, Loss: 1852.5807646740354\n",
      "Epoch 4/100, Loss: 1763.647459113954\n",
      "Epoch 5/100, Loss: 1653.818546022617\n",
      "Epoch 6/100, Loss: 1523.1037441670803\n",
      "Epoch 7/100, Loss: 1376.7680140414548\n",
      "Epoch 8/100, Loss: 1216.2174979198976\n",
      "Epoch 9/100, Loss: 998.7162701570958\n",
      "Epoch 10/100, Loss: 710.7936103057709\n",
      "Epoch 11/100, Loss: 481.2625826362743\n",
      "Epoch 12/100, Loss: 776.7947197484353\n",
      "Epoch 13/100, Loss: 671.6676085265377\n",
      "Epoch 14/100, Loss: 560.678172986824\n",
      "Epoch 15/100, Loss: 595.9643275539843\n",
      "Epoch 16/100, Loss: 638.2424866538337\n",
      "Epoch 17/100, Loss: 662.0370614323286\n",
      "Epoch 18/100, Loss: 666.7912564006713\n",
      "Epoch 19/100, Loss: 661.6483355428\n",
      "Epoch 20/100, Loss: 656.6941756157765\n",
      "Epoch 21/100, Loss: 660.681111130454\n",
      "Epoch 22/100, Loss: 659.640559832386\n",
      "Epoch 23/100, Loss: 649.0773018193964\n",
      "Epoch 24/100, Loss: 628.3424793030186\n",
      "Epoch 25/100, Loss: 605.1577066426185\n",
      "Epoch 26/100, Loss: 581.8776673964228\n",
      "Epoch 27/100, Loss: 560.358135157258\n",
      "Epoch 28/100, Loss: 535.004951758091\n",
      "Epoch 29/100, Loss: 519.1085988314388\n",
      "Epoch 30/100, Loss: 513.4430253987526\n",
      "Epoch 31/100, Loss: 500.61562711480997\n",
      "Epoch 32/100, Loss: 467.85200731903257\n",
      "Epoch 33/100, Loss: 416.87437819191325\n",
      "Epoch 34/100, Loss: 389.44170232096246\n",
      "Epoch 35/100, Loss: 391.27429716762805\n",
      "Epoch 36/100, Loss: 381.71215577676236\n",
      "Epoch 37/100, Loss: 389.00183373303724\n",
      "Epoch 38/100, Loss: 386.66927447481913\n",
      "Epoch 39/100, Loss: 392.06642456853456\n",
      "Epoch 40/100, Loss: 387.43057644751445\n",
      "Epoch 41/100, Loss: 381.1359761957367\n",
      "Epoch 42/100, Loss: 370.44550372350847\n",
      "Epoch 43/100, Loss: 350.08316447805805\n",
      "Epoch 44/100, Loss: 347.9417332870226\n",
      "Epoch 45/100, Loss: 341.89568513660095\n",
      "Epoch 46/100, Loss: 334.7388241592443\n",
      "Epoch 47/100, Loss: 326.4480543606844\n",
      "Epoch 48/100, Loss: 325.1265910634249\n",
      "Epoch 49/100, Loss: 327.54639120959206\n",
      "Epoch 50/100, Loss: 328.9029608153069\n",
      "Epoch 51/100, Loss: 324.89031825664694\n",
      "Epoch 52/100, Loss: 315.404955573499\n",
      "Epoch 53/100, Loss: 308.9505523309649\n",
      "Epoch 54/100, Loss: 314.1239867429796\n",
      "Epoch 55/100, Loss: 328.36522719162934\n",
      "Epoch 56/100, Loss: 310.73584123163175\n",
      "Epoch 57/100, Loss: 306.3275067371413\n",
      "Epoch 58/100, Loss: 306.2239084232995\n",
      "Epoch 59/100, Loss: 304.6936181040191\n",
      "Epoch 60/100, Loss: 306.99902595342206\n",
      "Epoch 61/100, Loss: 306.31862462148126\n",
      "Epoch 62/100, Loss: 306.1821223874354\n",
      "Epoch 63/100, Loss: 303.75380403492886\n",
      "Epoch 64/100, Loss: 297.14330445713375\n",
      "Epoch 65/100, Loss: 304.8846864127901\n",
      "Epoch 66/100, Loss: 303.95802541945335\n",
      "Epoch 67/100, Loss: 300.0166618312617\n",
      "Epoch 68/100, Loss: 293.98689335515695\n",
      "Epoch 69/100, Loss: 295.50847173563733\n",
      "Epoch 70/100, Loss: 300.8303795012185\n",
      "Epoch 71/100, Loss: 315.21693315889235\n",
      "Epoch 72/100, Loss: 314.5327254968822\n",
      "Epoch 73/100, Loss: 307.8688176898009\n",
      "Epoch 74/100, Loss: 315.72876659269053\n",
      "Epoch 75/100, Loss: 304.27393914359544\n",
      "Epoch 76/100, Loss: 316.28530207160776\n",
      "Epoch 77/100, Loss: 291.75189343158917\n",
      "Epoch 78/100, Loss: 315.215868649717\n",
      "Epoch 79/100, Loss: 296.676425884481\n",
      "Epoch 80/100, Loss: 317.8599451723181\n",
      "Epoch 81/100, Loss: 298.95112183563276\n",
      "Epoch 82/100, Loss: 294.06922884045207\n",
      "Epoch 83/100, Loss: 296.5802345163605\n",
      "Epoch 84/100, Loss: 291.4531441147197\n",
      "Epoch 85/100, Loss: 291.6135891003217\n",
      "Epoch 86/100, Loss: 290.2484276494989\n",
      "Epoch 87/100, Loss: 288.6159582910879\n",
      "Epoch 88/100, Loss: 289.9137243849375\n",
      "Epoch 89/100, Loss: 288.9883281745045\n",
      "Epoch 90/100, Loss: 284.2195629956483\n",
      "Epoch 91/100, Loss: 279.5183018967144\n",
      "Epoch 92/100, Loss: 282.1708155203726\n",
      "Epoch 93/100, Loss: 284.00672676068353\n",
      "Epoch 94/100, Loss: 284.9872489603898\n",
      "Epoch 95/100, Loss: 288.96279574513113\n",
      "Epoch 96/100, Loss: 283.38425453263557\n",
      "Epoch 97/100, Loss: nan\n",
      "Epoch 98/100, Loss: nan\n",
      "Epoch 99/100, Loss: nan\n",
      "Epoch 100/100, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assume these variables are already defined\n",
    "# thetas, beta, h, X_train, D_train, Y_train, mask, device\n",
    "\n",
    "thetas = torch.nn.Parameter(torch.normal(0, .1, size=(D_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "beta = torch.nn.Parameter(torch.normal(0, .1, size=(X_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "h=torch.nn.Parameter(torch.ones((1,Y_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = optim.Adam([thetas, beta, h], lr=.1)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# List to track the loss at each epoch\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients before backward pass\n",
    "    \n",
    "    # Forward pass\n",
    "    Z = (X_train @ beta) + (D_train @ thetas)\n",
    "    Z = Z @ torch.ones((1, Y_train.shape[1])).double()\n",
    "    K = k(Z[:, :, None] - Z.T[None, :, :], h)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = 0\n",
    "    for i in range(mask.shape[0]):\n",
    "        Y_hat = ((((K * Y_train.T[None, :, :])[i]) * mask[i]).sum(1) / \n",
    "                 ((K * Y_train.T[None, :, :])[i]).sum(1))\n",
    "        loss += (Y_train[i] - Y_hat).pow(2).sum()\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track the loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print the loss for the current epoch\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# losses now contain the loss value at each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b487bda8-9607-4f53-b2b0-d82ea918f581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Loss: 31.473319831205455, MAE: 0.00935490070458177\n",
      "Epoch 2/8, Loss: 30.566449114215132, MAE: 0.009160511832351973\n",
      "Epoch 3/8, Loss: 29.41860551448117, MAE: 0.008929719264447232\n",
      "Epoch 4/8, Loss: 28.046466475321424, MAE: 0.008671480869101453\n",
      "Epoch 5/8, Loss: 26.28469016783835, MAE: 0.008329395175142687\n",
      "Epoch 6/8, Loss: 24.091191853094287, MAE: 0.007871639494972653\n",
      "Epoch 7/8, Loss: 21.494921377501303, MAE: 0.0073034614541404525\n",
      "Epoch 8/8, Loss: 18.683643231975825, MAE: 0.0067661822345306604\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assume these variables are already defined\n",
    "# thetas, beta, h, X_train, D_train, Y_train, mask, device\n",
    "\n",
    "thetas = torch.nn.Parameter(torch.normal(0, .1, size=(D_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "beta = torch.nn.Parameter(torch.normal(0, .1, size=(X_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "h = torch.nn.Parameter(torch.ones((1,Y_train.shape[1],1), dtype=torch.float64, device=device))\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = optim.Adam([thetas, beta, h], lr=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 8\n",
    "\n",
    "# Lists to track the loss and MAE at each epoch\n",
    "losses = []\n",
    "maes = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Zero the gradients before backward pass\n",
    "    \n",
    "    # Forward pass\n",
    "    Z = (X_train @ beta) + (D_train @ thetas)\n",
    "    Z = Z @ torch.ones((1, Y_train.shape[1])).double()\n",
    "    K = k(Z[:, :, None] - Z.T[None, :, :], h)\n",
    "    \n",
    "    # Compute the loss and MAE\n",
    "    loss = 0\n",
    "    mae = 0\n",
    "    for i in range(mask.shape[0]):\n",
    "        Y_hat = ((((K * Y_train.T[None, :, :])[i]) * mask[i]).sum(1) / \n",
    "                 ((K * Y_train.T[None, :, :])[i]).sum(1))\n",
    "        loss += (Y_train[i] - Y_hat).pow(2).mean()\n",
    "        mae += torch.abs(Y_train[i] - Y_hat).mean()\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track the loss and MAE\n",
    "    losses.append(loss.item()/ Y_train.numel())\n",
    "    maes.append(mae.item() / Y_train.numel())  # Average MAE across all elements\n",
    "    \n",
    "    # Print the loss and MAE for the current epoch\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, MAE: {maes[-1]}')\n",
    "\n",
    "# losses and maes now contain the loss and MAE values at each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "45b8f06b-6a3a-46ae-85d9-1c8adabed1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6541, 0.6617, 0.6543, 0.7101, 0.7642, 0.6744, 0.6417, 0.6379, 0.6524,\n",
       "        0.6362, 0.6326, 0.6702, 0.6604, 0.7055, 0.6473, 0.6466, 0.6692, 0.6438,\n",
       "        0.6502, 0.6528, 0.6461, 0.6810, 0.6134, 0.6404, 0.6498, 0.6607, 0.6397,\n",
       "        0.6409, 0.6426, 0.6460, 0.7045, 0.6461, 0.6746, 0.6466, 0.6514, 0.6286,\n",
       "        0.6467, 0.6395, 0.6422, 0.6502, 0.6380, 0.6379, 0.6543, 0.6589, 0.6595,\n",
       "        0.6338, 0.6445, 0.6496, 0.5891, 0.6401, 0.6447, 0.6423, 0.6475, 0.6369,\n",
       "        0.6178, 0.6269, 0.6608, 0.6521, 0.6523, 0.6463, 0.6577, 0.6566],\n",
       "       dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b40d50fc-0ae1-45ca-98a2-c951dcdee691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6578, 0.8206, 0.8287, 0.4260, 0.3487, 0.4200, 0.7333, 0.8947, 0.7800,\n",
       "        0.8113, 0.8819, 0.4700, 0.6300, 0.4000, 0.7110, 0.5490, 0.8000, 0.7745,\n",
       "        0.5614, 0.7724, 0.6455, 0.5159, 0.5700, 0.8968, 0.6502, 0.8200, 0.8485,\n",
       "        0.9293, 0.9741, 0.7821, 0.3815, 0.8403, 0.4636, 0.9211, 0.6713, 0.9069,\n",
       "        0.9072, 0.7937, 0.8626, 0.9174, 0.8889, 0.8834, 0.5804, 0.8932, 0.9402,\n",
       "        0.8800, 0.9029, 0.8035, 0.6760, 0.8464, 0.8199, 0.8364, 0.6135, 0.5913,\n",
       "        0.8088, 0.8203, 0.7273, 0.8204, 0.8806, 0.9200, 0.5723, 0.8772],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202b316-4fc0-45c0-8c7f-22c42e710464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1e5b6-098b-4ac8-8dea-a3d022e9c057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff93ab-6a08-4dc3-85ce-054b4ed88b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471cfbc-d9c8-45e8-9efd-8a7d093fc5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e51beb2-1a9c-4041-8841-2c3e9faaaf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e35ee4-8b85-44c3-8199-ef62a9538396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883351a8-7183-4adf-80e3-8b6e8ea20076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, use_torch=True):\n",
    "    if use_torch:\n",
    "        return torch.nn.Sigmoid()(z)\n",
    "    else:\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66fe85a9-e9bc-4113-be54-8ae53b851066",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "\n",
    "epochs=100\n",
    "tol=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "266055cd-7279-417b-94dd-c1f34eaa1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90175b7b-a90a-4656-ab88-605394f4459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how='any')\n",
    "unique_families, counts_families = np.unique(data.family, return_counts=True)\n",
    "avail_families = unique_families[counts_families>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b267bfd4-c75e-4092-89ed-75c9e83b77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e697050caf694e6aab00fb933308ffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "preds_baseline = []\n",
    "preds_baseline2 = []\n",
    "preds_rf = []\n",
    "ys = []\n",
    "\n",
    "for test_family in tqdm(avail_families):\n",
    "    \n",
    "    data_train = data.loc[data.family != test_family]\n",
    "    data_test = data.loc[data.family == test_family]\n",
    "    data_train = pd.concat((data_test.iloc[:1],data_train), axis=0).reset_index(drop=True)\n",
    "    data_test = data_test.iloc[1:].reset_index(drop=True)\n",
    "    \n",
    "    ###\n",
    "    Y_train = torch.tensor(logit(np.array(data_train.loc[:,['ARC-C','HellaSwag','Winograd','TruthfulQA','GSM8K']+mmlu_subs])))\n",
    "    X_train = np.array(data_train.loc[:,['logT','logS','logS*logT']])\n",
    "    X_train = torch.tensor(np.hstack((np.ones(X_train.shape[0])[:,None], X_train))).double()\n",
    "    Y_test = torch.tensor(logit(np.array(data_test.loc[:,['ARC-C','HellaSwag','Winograd','TruthfulQA','GSM8K']+mmlu_subs])))\n",
    "    X_test = np.array(data_test.loc[:,['logT','logS','logS*logT']])\n",
    "    X_test = torch.tensor(np.hstack((np.ones(X_test.shape[0])[:,None], X_test))).double()\n",
    "    D_train = torch.tensor(np.array(pd.get_dummies(np.array(data_train.family)))).double()\n",
    "    D_test = torch.tensor(np.vstack([D_train[0,:].numpy() for _ in range(Y_test.shape[0])])).double()\n",
    "\n",
    "    X2_train = torch.hstack((torch.tensor(np.array(data_train.loc[:,['logT','logS','logS*logT']])), D_train)) #torch.tensor(np.array(data_train.loc[:,['logF']]))\n",
    "    X2_test = torch.hstack((torch.tensor(np.array(data_test.loc[:,['logT','logS','logS*logT']])), D_test)) #torch.tensor(np.array(data_train.loc[:,['logF']]))\n",
    "\n",
    "    ###\n",
    "    thetas = torch.nn.Parameter(torch.normal(0, .5, size=(D_train.shape[1],d), dtype=torch.float64, device=device))\n",
    "    beta = torch.nn.Parameter(torch.normal(0, .5, size=(X_train.shape[1]+d,Y_train.shape[1]), dtype=torch.float64, device=device))\n",
    "    beta_baseline = torch.nn.Parameter(torch.normal(0, .5, size=(X_train.shape[1],Y_train.shape[1]), dtype=torch.float64, device=device))\n",
    "    beta_baseline2 = torch.nn.Parameter(torch.normal(0, .5, size=(X2_train.shape[1],Y_train.shape[1]), dtype=torch.float64, device=device))\n",
    "    \n",
    "    ###\n",
    "    optimizer = torch.optim.LBFGS([beta,thetas], lr=.1, line_search_fn='strong_wolfe')\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = (((X_train@beta[d:] + (D_train@thetas)@beta[:d])-Y_train)**2).mean()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        abs_loss = (((X_train@beta[d:] + (D_train@thetas)@beta[:d])-Y_train).abs()).mean()\n",
    "        losses.append(abs_loss.item())\n",
    "    \n",
    "        if epoch>=1:\n",
    "            if losses[-2]-losses[-1]<=tol:\n",
    "                break\n",
    "\n",
    "    ###\n",
    "    optimizer = torch.optim.LBFGS([beta_baseline], lr=.1, line_search_fn='strong_wolfe')\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = (((X_train@beta_baseline)-Y_train)**2).mean()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        abs_loss = (((X_train@beta_baseline)-Y_train).abs()).mean()\n",
    "        losses.append(abs_loss.item())\n",
    "    \n",
    "        if epoch>=1:\n",
    "            if losses[-2]-losses[-1]<=tol:\n",
    "                break\n",
    "\n",
    "    ###\n",
    "    optimizer = torch.optim.LBFGS([beta_baseline2], lr=.1, line_search_fn='strong_wolfe')\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = (((X2_train@beta_baseline2)-Y_train)**2).mean()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        loss = optimizer.step(closure)\n",
    "        abs_loss = (((X2_train@beta_baseline2)-Y_train).abs()).mean()\n",
    "        losses.append(abs_loss.item())\n",
    "    \n",
    "        if epoch>=1:\n",
    "            if losses[-2]-losses[-1]<=tol:\n",
    "                break\n",
    "\n",
    "    preds.append(sigmoid(((X_test@beta[d:] + (D_test@thetas)@beta[:d]))).detach().numpy())\n",
    "    preds_baseline.append(sigmoid(((X_test@beta_baseline))).detach().numpy())\n",
    "    preds_baseline2.append(sigmoid(((X2_test@beta_baseline2))).detach().numpy())\n",
    "\n",
    "    ###\n",
    "    cols = [[0],[1],[2],[3],[4],list(range(Y_train.shape[1]))[5:]]\n",
    "    pred_rf = np.zeros((Y_test.shape[0],Y_train.shape[1]))\n",
    "    for col in cols:\n",
    "        regr = RidgeCV(alphas=np.linspace(1e-5,10,20)) #RandomForestRegressor(n_estimators=1000, max_features='sqrt', random_state=0)\n",
    "        regr.fit(Y_train[:,:4], Y_train[:,col].squeeze()) #np.delete(sigmoid(Y_train), col, axis=1), \n",
    "        pred_rf[:,col] = sigmoid(regr.predict(logit(preds_baseline2[-1])[:,:4]).reshape(-1,len(col)), use_torch=False) #np.delete(logit(preds_baseline2[-1]), col, axis=1)\n",
    "        #pred_rf[:,[i for i in range(Y_train.shape[1]) if i not in col]] = preds_baseline2[-1][:,[i for i in range(Y_train.shape[1]) if i not in col]]\n",
    "    preds_rf.append(pred_rf)\n",
    "        \n",
    "    ys.append(sigmoid(Y_test[:,:]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83a287d5-5278-42d4-b0e6-76979ed15012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03484117, 0.04405401, 0.02929322, 0.04246133, 0.05998127])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c3f764-781c-404b-b73e-c6a6f6dbbc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05159197, 0.06620617, 0.04097824, 0.03716612, 0.0777711 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds_baseline)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8f6d75-9ac6-4a98-865c-3a235942c8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02932462, 0.046043  , 0.02988084, 0.04813632, 0.06037273])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds_baseline2)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f4e24c-ac34-4564-8a61-34111f0bee70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02936353, 0.04601182, 0.02983732, 0.04809264, 0.09816683])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds_rf)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c92c9-a6e9-4c6a-bf9a-fe82e64b98d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0354c34-76f4-4e3e-bda3-9754250f6c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02349686, 0.029052  , 0.02275559, 0.03488756, 0.01179707])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a6367e-5f3f-4cfc-9807-7f9c10db8ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03058035, 0.04576952, 0.02934638, 0.02100441, 0.02115813])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds_baseline)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b5651d-e055-4cd3-b2aa-350814b0bc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01784383, 0.03382038, 0.02087774, 0.04381062, 0.01115699])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds_baseline2)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2c3caa3-d9d6-4053-86d2-b4c654b7fe28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01797291, 0.03410773, 0.02103038, 0.04392663, 0.02196258])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds_rf)[:,:5]-np.vstack(ys)[:,:5]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930fd07-17d2-44e8-b362-1b6e0ac127c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196d2e4-0366-48b2-9d9f-5c6d8db0dbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53448c99-91c4-447b-bbbd-00f1cc9173d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05534097995038255"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f18d3af-a8ac-45f7-96d3-5ad223c6b28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07484974350315568"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds_baseline)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99d6597f-f137-4d5a-bd81-473ac40b4d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05556360186100937"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds_baseline2)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a578fc5b-8e59-4f04-a352-f1ce46df403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07491827144189878"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.vstack(preds_rf)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff9f9e-eaa7-4c7d-93f6-0d93f279dcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff042c57-c7b8-403f-a94e-12ea2d8d66f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04061560915989798"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15026573-0bb3-425f-adf6-fd8ceab7789d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07036375818993645"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds_baseline)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc2efb47-8250-4131-96a8-610f86e84aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04036635411883588"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(np.abs(np.vstack(preds_baseline2)[:,5:].mean(1)-np.vstack(ys)[:,5:].mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cdf07-d10f-4e28-8758-afa4261eacc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e913971-732a-420f-b2c1-1538fb08a393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
